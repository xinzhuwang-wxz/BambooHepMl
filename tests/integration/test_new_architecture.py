"""
æ–°æ¶æ„é›†æˆæµ‹è¯•

æµ‹è¯•æ–° MLP æ¶æ„çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. åªæœ‰ event ç‰¹å¾çš„æƒ…å†µ
2. åªæœ‰ object ç‰¹å¾çš„æƒ…å†µ
3. åŒæ—¶æœ‰ event å’Œ object ç‰¹å¾çš„æƒ…å†µ
4. æ•´ä¸ª pipelineï¼šdata â†’ model â†’ train â†’ eval â†’ export â†’ serve

é‡ç‚¹éªŒè¯ï¼š
- ç»´åº¦è‡ªåŠ¨æ¨æ–­
- æ•°æ®æ ¼å¼ï¼ˆevent/object/maskï¼‰çš„æ­£ç¡®æ€§
- ç«¯åˆ°ç«¯æµç¨‹çš„å®Œæ•´æ€§
"""

import os
import sys
import tempfile
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
os.environ["PYTHONPATH"] = str(project_root) + os.pathsep + os.environ.get("PYTHONPATH", "")

import awkward as ak  # noqa: E402
import numpy as np  # noqa: E402
import pytest  # noqa: E402
import torch  # noqa: E402
import yaml  # noqa: E402
from torch.utils.data import DataLoader  # noqa: E402

from bamboohepml.data import DataConfig, DataSourceFactory, HEPDataset  # noqa: E402
from bamboohepml.data.features import ExpressionEngine, FeatureGraph  # noqa: E402
from bamboohepml.engine import Evaluator, Trainer  # noqa: E402
from bamboohepml.models import get_model  # noqa: E402
from bamboohepml.utils import collate_fn  # noqa: E402


def print_section(title: str):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def print_batch_info(batch: dict, title: str = "Batch ä¿¡æ¯"):
    """æ‰“å° batch çš„è¯¦ç»†ä¿¡æ¯"""
    print(f"\n{title}:")
    print(f"  é”®: {list(batch.keys())}")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            # å¯¹äºéæµ®ç‚¹ç±»å‹çš„ tensorï¼ˆå¦‚ int64, boolï¼‰ï¼Œä¸èƒ½è®¡ç®— mean
            try:
                mean_val = value.float().mean().item()
                min_val = value.min().item()
                max_val = value.max().item()
                print(f"    {key}: shape={value.shape}, dtype={value.dtype}, min={min_val:.4f}, max={max_val:.4f}, mean={mean_val:.4f}")
            except (RuntimeError, TypeError):
                # å¯¹äº bool æˆ–æŸäº›ç±»å‹ï¼Œåªæ‰“å° shape å’Œ dtype
                print(f"    {key}: shape={value.shape}, dtype={value.dtype}")
        elif isinstance(value, ak.Array):
            print(f"    {key}: type=ak.Array, length={len(value)}")
        else:
            print(f"    {key}: type={type(value).__name__}, value={value}")


def create_mock_data_source(num_events: int = 100):
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æº"""
    # åˆ›å»º event-level æ•°æ®
    met = np.random.uniform(0, 500, num_events)
    ht = np.random.uniform(0, 1000, num_events)

    # åˆ›å»º object-level æ•°æ®ï¼ˆjagged arrayï¼‰
    num_jets_per_event = np.random.randint(2, 8, num_events)
    jet_pt = []
    jet_eta = []
    for n in num_jets_per_event:
        jet_pt.append(np.random.uniform(20, 200, n))
        jet_eta.append(np.random.uniform(-2.5, 2.5, n))

    # åˆ›å»ºæ ‡ç­¾
    labels = np.random.randint(0, 2, num_events)

    # æ„å»º awkward array
    data = {
        "met": met,
        "ht": ht,
        "Jet_pt": ak.Array(jet_pt),
        "Jet_eta": ak.Array(jet_eta),
        "is_signal": labels,
    }

    table = ak.Array(data)

    # åˆ›å»º MockDataSource
    class MockDataSource:
        def __init__(self, table):
            self.table = table

        def load_branches(self, branches):
            if not branches:
                return self.table
            result = {}
            for branch in branches:
                if branch in self.table.fields:
                    result[branch] = self.table[branch]
            if not result:
                return self.table  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†æ”¯ï¼Œè¿”å›æ•´ä¸ªè¡¨
            return ak.Array(result)

        def get_num_events(self):
            return len(self.table)

        def get_available_branches(self):
            return list(self.table.fields)

    return MockDataSource(table)


def test_only_event_features():
    """æµ‹è¯•åªæœ‰ event-level ç‰¹å¾çš„æƒ…å†µ"""
    print_section("æµ‹è¯• 1: åªæœ‰ Event-Level ç‰¹å¾")

    # åˆ›å»º FeatureGraphï¼ˆåªæœ‰ event ç‰¹å¾ï¼‰
    feature_defs = {
        "met": {
            "expr": "met",
            "type": "event",
            "dtype": "float32",
        },
        "ht": {
            "expr": "ht",
            "type": "event",
            "dtype": "float32",
        },
    }

    engine = ExpressionEngine()
    feature_graph = FeatureGraph.from_feature_defs(feature_defs, engine, enable_cache=False)
    feature_graph.compile()

    # åˆ›å»ºæ•°æ®
    data_source = create_mock_data_source(num_events=50)
    table = data_source.load_branches(["met", "ht", "is_signal"])

    # æ‹Ÿåˆ FeatureGraph
    print("\n1. æ‹Ÿåˆ FeatureGraph...")
    feature_graph.fit(table)
    print("   âœ“ FeatureGraph æ‹ŸåˆæˆåŠŸ")

    # æ£€æŸ¥ output_spec
    print("\n2. æ£€æŸ¥ FeatureGraph.output_spec()...")
    output_spec = feature_graph.output_spec()
    print(f"   output_spec: {output_spec}")
    assert "event" in output_spec, "åº”è¯¥åŒ…å« event ç‰¹å¾"
    assert "object" not in output_spec, "ä¸åº”è¯¥åŒ…å« object ç‰¹å¾"
    print(f"   event dim: {output_spec['event']['dim']}")
    print(f"   event features: {output_spec['event']['features']}")

    # æ„å»º batch
    print("\n3. æ„å»º batch...")
    batch = feature_graph.build_batch(table)
    print_batch_info(batch, "FeatureGraph.build_batch() è¾“å‡º")

    assert "event" in batch, "batch åº”è¯¥åŒ…å« 'event' é”®"
    assert "object" not in batch, "batch ä¸åº”è¯¥åŒ…å« 'object' é”®"
    assert batch["event"].shape[1] == 2, f"event ç»´åº¦åº”è¯¥æ˜¯ 2ï¼Œå®é™…æ˜¯ {batch['event'].shape[1]}"
    print("   âœ“ Batch æ ¼å¼æ­£ç¡®")

    # åˆ›å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ç»´åº¦ï¼‰
    print("\n4. åˆ›å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ç»´åº¦ï¼‰...")
    event_input_dim = output_spec["event"]["dim"]
    model = get_model(
        "mlp_classifier",
        event_input_dim=event_input_dim,
        object_input_dim=None,
        embed_dim=64,
        hidden_dims=[32, 16],
        num_classes=2,
    )
    print(f"   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   event_input_dim: {event_input_dim}")
    print(f"   object_input_dim: None")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n5. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")

    model.eval()
    with torch.no_grad():
        output = model(batch)
    print(f"   è¾“å…¥ event shape: {batch['event'].shape}")
    print(f"   è¾“å‡º shape: {output.shape}")
    print(f"   è¾“å‡ºå€¼ï¼ˆå‰5ä¸ªï¼‰: {output[:5]}")
    assert output.shape == (len(table), 2), f"è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ ({len(table)}, 2)ï¼Œå®é™…æ˜¯ {output.shape}"
    print("   âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•è®­ç»ƒ
    print("\n6. æµ‹è¯•è®­ç»ƒæµç¨‹...")
    data_config = DataConfig(
        selection=None,
        labels={"type": "simple", "value": ["is_signal"]},
    )
    dataset = HEPDataset(
        data_source=data_source,
        data_config=data_config,
        feature_graph=feature_graph,
        for_training=True,
        shuffle=False,
    )

    loader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)

    # æ£€æŸ¥ DataLoader è¾“å‡ºçš„ batch æ ¼å¼
    sample_batch = next(iter(loader))
    print_batch_info(sample_batch, "DataLoader è¾“å‡ºçš„ Batch")

    trainer = Trainer(
        model=model,
        train_loader=loader,
        loss_fn=torch.nn.CrossEntropyLoss(),
        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),
        device=torch.device("cpu"),
        task_type="classification",
    )

    # è®­ç»ƒä¸€ä¸ª epoch
    history = trainer.fit(num_epochs=1)
    print(f"   âœ“ è®­ç»ƒå®Œæˆï¼Œhistory keys: {list(history.keys())}")

    # æµ‹è¯•è¯„ä¼°
    print("\n7. æµ‹è¯•è¯„ä¼°æµç¨‹...")
    evaluator = Evaluator(task_type="classification")
    metrics = evaluator.evaluate(model, loader, loss_fn=torch.nn.CrossEntropyLoss(), device=torch.device("cpu"))
    print(f"   è¯„ä¼°æŒ‡æ ‡: {metrics}")
    print("   âœ“ è¯„ä¼°æµ‹è¯•é€šè¿‡")

    print("\nâœ“ æµ‹è¯• 1 å®Œæˆï¼šåªæœ‰ Event-Level ç‰¹å¾")


def test_only_object_features():
    """æµ‹è¯•åªæœ‰ object-level ç‰¹å¾çš„æƒ…å†µ"""
    print_section("æµ‹è¯• 2: åªæœ‰ Object-Level ç‰¹å¾")

    # åˆ›å»º FeatureGraphï¼ˆåªæœ‰ object ç‰¹å¾ï¼‰
    feature_defs = {
        "jet_pt": {
            "expr": "Jet_pt",
            "type": "object",
            "dtype": "float32",
            "padding": {
                "max_length": 10,
                "mode": "constant",
            },
        },
        "jet_eta": {
            "expr": "Jet_eta",
            "type": "object",
            "dtype": "float32",
            "padding": {
                "max_length": 10,
                "mode": "constant",
            },
        },
    }

    engine = ExpressionEngine()
    feature_graph = FeatureGraph.from_feature_defs(feature_defs, engine, enable_cache=False)
    feature_graph.compile()

    # åˆ›å»ºæ•°æ®
    data_source = create_mock_data_source(num_events=50)
    table = data_source.load_branches(["Jet_pt", "Jet_eta", "is_signal"])

    # æ‹Ÿåˆ FeatureGraph
    print("\n1. æ‹Ÿåˆ FeatureGraph...")
    feature_graph.fit(table)
    print("   âœ“ FeatureGraph æ‹ŸåˆæˆåŠŸ")

    # æ£€æŸ¥ output_spec
    print("\n2. æ£€æŸ¥ FeatureGraph.output_spec()...")
    output_spec = feature_graph.output_spec()
    print(f"   output_spec: {output_spec}")
    assert "object" in output_spec, "åº”è¯¥åŒ…å« object ç‰¹å¾"
    assert "event" not in output_spec, "ä¸åº”è¯¥åŒ…å« event ç‰¹å¾"
    print(f"   object dim: {output_spec['object']['dim']}")
    print(f"   object max_length: {output_spec['object']['max_length']}")
    print(f"   object features: {output_spec['object']['features']}")

    # æ„å»º batch
    print("\n3. æ„å»º batch...")
    batch = feature_graph.build_batch(table)
    print_batch_info(batch, "FeatureGraph.build_batch() è¾“å‡º")

    assert "object" in batch, "batch åº”è¯¥åŒ…å« 'object' é”®"
    assert "mask" in batch, "batch åº”è¯¥åŒ…å« 'mask' é”®"
    assert "event" not in batch, "batch ä¸åº”è¯¥åŒ…å« 'event' é”®"
    assert batch["object"].shape == (len(table), 10, 2), f"object å½¢çŠ¶åº”è¯¥æ˜¯ ({len(table)}, 10, 2)ï¼Œå®é™…æ˜¯ {batch['object'].shape}"
    assert batch["mask"].shape == (len(table), 10), f"mask å½¢çŠ¶åº”è¯¥æ˜¯ ({len(table)}, 10)ï¼Œå®é™…æ˜¯ {batch['mask'].shape}"
    print("   âœ“ Batch æ ¼å¼æ­£ç¡®")

    # åˆ›å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ç»´åº¦ï¼‰
    print("\n4. åˆ›å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ç»´åº¦ï¼‰...")
    object_input_dim = output_spec["object"]["dim"]
    model = get_model(
        "mlp_classifier",
        event_input_dim=None,
        object_input_dim=object_input_dim,
        embed_dim=64,
        hidden_dims=[32, 16],
        num_classes=2,
        object_pooling_mode="mean",
    )
    print(f"   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   event_input_dim: None")
    print(f"   object_input_dim: {object_input_dim}")
    print(f"   object_pooling_mode: mean")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n5. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
    model.eval()
    with torch.no_grad():
        output = model(batch)
    print(f"   è¾“å…¥ object shape: {batch['object'].shape}")
    print(f"   è¾“å…¥ mask shape: {batch['mask'].shape}")
    print(f"   è¾“å‡º shape: {output.shape}")
    print(f"   è¾“å‡ºå€¼ï¼ˆå‰5ä¸ªï¼‰: {output[:5]}")
    assert output.shape == (len(table), 2), f"è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ ({len(table)}, 2)ï¼Œå®é™…æ˜¯ {output.shape}"
    print("   âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•è®­ç»ƒ
    print("\n6. æµ‹è¯•è®­ç»ƒæµç¨‹...")
    data_config = DataConfig(
        selection=None,
        labels={"type": "simple", "value": ["is_signal"]},
    )
    dataset = HEPDataset(
        data_source=data_source,
        data_config=data_config,
        feature_graph=feature_graph,
        for_training=True,
        shuffle=False,
    )

    loader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)

    # æ£€æŸ¥ DataLoader è¾“å‡ºçš„ batch æ ¼å¼
    sample_batch = next(iter(loader))
    print_batch_info(sample_batch, "DataLoader è¾“å‡ºçš„ Batch")

    trainer = Trainer(
        model=model,
        train_loader=loader,
        loss_fn=torch.nn.CrossEntropyLoss(),
        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),
        device=torch.device("cpu"),
        task_type="classification",
    )

    # è®­ç»ƒä¸€ä¸ª epoch
    history = trainer.fit(num_epochs=1)
    print(f"   âœ“ è®­ç»ƒå®Œæˆï¼Œhistory keys: {list(history.keys())}")

    # æµ‹è¯•è¯„ä¼°
    print("\n7. æµ‹è¯•è¯„ä¼°æµç¨‹...")
    evaluator = Evaluator(task_type="classification")
    metrics = evaluator.evaluate(model, loader, loss_fn=torch.nn.CrossEntropyLoss(), device=torch.device("cpu"))
    print(f"   è¯„ä¼°æŒ‡æ ‡: {metrics}")
    print("   âœ“ è¯„ä¼°æµ‹è¯•é€šè¿‡")

    print("\nâœ“ æµ‹è¯• 2 å®Œæˆï¼šåªæœ‰ Object-Level ç‰¹å¾")


def test_both_event_and_object_features():
    """æµ‹è¯•åŒæ—¶æœ‰ event å’Œ object ç‰¹å¾çš„æƒ…å†µ"""
    print_section("æµ‹è¯• 3: Event + Object ç‰¹å¾")

    # åˆ›å»º FeatureGraphï¼ˆåŒæ—¶æœ‰ event å’Œ object ç‰¹å¾ï¼‰
    feature_defs = {
        "met": {
            "expr": "met",
            "type": "event",
            "dtype": "float32",
        },
        "ht": {
            "expr": "ht",
            "type": "event",
            "dtype": "float32",
        },
        "jet_pt": {
            "expr": "Jet_pt",
            "type": "object",
            "dtype": "float32",
            "padding": {
                "max_length": 10,
                "mode": "constant",
            },
        },
        "jet_eta": {
            "expr": "Jet_eta",
            "type": "object",
            "dtype": "float32",
            "padding": {
                "max_length": 10,
                "mode": "constant",
            },
        },
    }

    engine = ExpressionEngine()
    feature_graph = FeatureGraph.from_feature_defs(feature_defs, engine, enable_cache=False)
    feature_graph.compile()

    # åˆ›å»ºæ•°æ®
    data_source = create_mock_data_source(num_events=50)
    table = data_source.load_branches(["met", "ht", "Jet_pt", "Jet_eta", "is_signal"])

    # æ‹Ÿåˆ FeatureGraph
    print("\n1. æ‹Ÿåˆ FeatureGraph...")
    feature_graph.fit(table)
    print("   âœ“ FeatureGraph æ‹ŸåˆæˆåŠŸ")

    # æ£€æŸ¥ output_spec
    print("\n2. æ£€æŸ¥ FeatureGraph.output_spec()...")
    output_spec = feature_graph.output_spec()
    print(f"   output_spec: {output_spec}")
    assert "event" in output_spec, "åº”è¯¥åŒ…å« event ç‰¹å¾"
    assert "object" in output_spec, "åº”è¯¥åŒ…å« object ç‰¹å¾"
    print(f"   event dim: {output_spec['event']['dim']}")
    print(f"   event features: {output_spec['event']['features']}")
    print(f"   object dim: {output_spec['object']['dim']}")
    print(f"   object max_length: {output_spec['object']['max_length']}")
    print(f"   object features: {output_spec['object']['features']}")

    # æ„å»º batch
    print("\n3. æ„å»º batch...")
    batch = feature_graph.build_batch(table)
    print_batch_info(batch, "FeatureGraph.build_batch() è¾“å‡º")

    assert "event" in batch, "batch åº”è¯¥åŒ…å« 'event' é”®"
    assert "object" in batch, "batch åº”è¯¥åŒ…å« 'object' é”®"
    assert "mask" in batch, "batch åº”è¯¥åŒ…å« 'mask' é”®"
    assert batch["event"].shape[1] == 2, f"event ç»´åº¦åº”è¯¥æ˜¯ 2ï¼Œå®é™…æ˜¯ {batch['event'].shape[1]}"
    assert batch["object"].shape == (len(table), 10, 2), f"object å½¢çŠ¶åº”è¯¥æ˜¯ ({len(table)}, 10, 2)ï¼Œå®é™…æ˜¯ {batch['object'].shape}"
    assert batch["mask"].shape == (len(table), 10), f"mask å½¢çŠ¶åº”è¯¥æ˜¯ ({len(table)}, 10)ï¼Œå®é™…æ˜¯ {batch['mask'].shape}"
    print("   âœ“ Batch æ ¼å¼æ­£ç¡®")

    # åˆ›å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ç»´åº¦ï¼‰
    print("\n4. åˆ›å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ç»´åº¦ï¼‰...")
    event_input_dim = output_spec["event"]["dim"]
    object_input_dim = output_spec["object"]["dim"]
    model = get_model(
        "mlp_classifier",
        event_input_dim=event_input_dim,
        object_input_dim=object_input_dim,
        embed_dim=64,
        hidden_dims=[32, 16],
        num_classes=2,
        object_pooling_mode="mean",
    )
    print(f"   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   event_input_dim: {event_input_dim}")
    print(f"   object_input_dim: {object_input_dim}")
    print(f"   embed_dim: 64")
    print(f"   object_pooling_mode: mean")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n5. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
    model.eval()
    with torch.no_grad():
        output = model(batch)
    print(f"   è¾“å…¥ event shape: {batch['event'].shape}")
    print(f"   è¾“å…¥ object shape: {batch['object'].shape}")
    print(f"   è¾“å…¥ mask shape: {batch['mask'].shape}")
    print(f"   è¾“å‡º shape: {output.shape}")
    print(f"   è¾“å‡ºå€¼ï¼ˆå‰5ä¸ªï¼‰: {output[:5]}")
    assert output.shape == (len(table), 2), f"è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ ({len(table)}, 2)ï¼Œå®é™…æ˜¯ {output.shape}"
    print("   âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•è®­ç»ƒ
    print("\n6. æµ‹è¯•è®­ç»ƒæµç¨‹...")
    data_config = DataConfig(
        selection=None,
        labels={"type": "simple", "value": ["is_signal"]},
    )
    dataset = HEPDataset(
        data_source=data_source,
        data_config=data_config,
        feature_graph=feature_graph,
        for_training=True,
        shuffle=False,
    )

    loader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)

    # æ£€æŸ¥ DataLoader è¾“å‡ºçš„ batch æ ¼å¼
    sample_batch = next(iter(loader))
    print_batch_info(sample_batch, "DataLoader è¾“å‡ºçš„ Batch")

    trainer = Trainer(
        model=model,
        train_loader=loader,
        loss_fn=torch.nn.CrossEntropyLoss(),
        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),
        device=torch.device("cpu"),
        task_type="classification",
    )

    # è®­ç»ƒä¸€ä¸ª epoch
    history = trainer.fit(num_epochs=1)
    print(f"   âœ“ è®­ç»ƒå®Œæˆï¼Œhistory keys: {list(history.keys())}")

    # æµ‹è¯•è¯„ä¼°
    print("\n7. æµ‹è¯•è¯„ä¼°æµç¨‹...")
    evaluator = Evaluator(task_type="classification")
    metrics = evaluator.evaluate(model, loader, loss_fn=torch.nn.CrossEntropyLoss(), device=torch.device("cpu"))
    print(f"   è¯„ä¼°æŒ‡æ ‡: {metrics}")
    print("   âœ“ è¯„ä¼°æµ‹è¯•é€šè¿‡")

    print("\nâœ“ æµ‹è¯• 3 å®Œæˆï¼šEvent + Object ç‰¹å¾")


def test_pipeline_orchestrator_auto_inference():
    """æµ‹è¯• PipelineOrchestrator çš„è‡ªåŠ¨ç»´åº¦æ¨æ–­"""
    print_section("æµ‹è¯• 4: PipelineOrchestrator è‡ªåŠ¨ç»´åº¦æ¨æ–­")

    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)

        # åˆ›å»º pipeline.yaml é…ç½®
        pipeline_config = {
            "data": {
                "source": {
                    "type": "mock",
                    "path": "dummy",
                },
            },
            "features": {
                "config_path": str(tmpdir / "features.yaml"),
            },
            "model": {
                "name": "mlp_classifier",
                "params": {
                    "embed_dim": 64,
                    "hidden_dims": [32, 16],
                    "num_classes": 2,
                    "object_pooling_mode": "mean",
                    # ä¸æŒ‡å®š event_input_dim å’Œ object_input_dimï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨æ¨æ–­
                },
            },
            "train": {
                "num_epochs": 1,
                "batch_size": 8,
                "task_type": "classification",
            },
        }

        # åˆ›å»º features.yaml
        feature_defs = {
            "met": {
                "expr": "met",
                "type": "event",
                "dtype": "float32",
            },
            "jet_pt": {
                "expr": "Jet_pt",
                "type": "object",
                "dtype": "float32",
                "padding": {
                    "max_length": 10,
                    "mode": "constant",
                },
            },
        }

        with open(tmpdir / "features.yaml", "w") as f:
            yaml.dump({"features": feature_defs}, f)

        with open(tmpdir / "pipeline.yaml", "w") as f:
            yaml.dump(pipeline_config, f)

        print(f"\n1. Pipeline é…ç½®æ–‡ä»¶: {tmpdir / 'pipeline.yaml'}")
        print(f"   Features é…ç½®æ–‡ä»¶: {tmpdir / 'features.yaml'}")

        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸èƒ½çœŸæ­£è¿è¡Œ PipelineOrchestratorï¼Œå› ä¸ºå®ƒéœ€è¦çœŸå®çš„æ•°æ®æº
        # ä½†æˆ‘ä»¬å¯ä»¥æµ‹è¯•ç»´åº¦æ¨æ–­çš„é€»è¾‘

        # åˆ›å»º FeatureGraph å¹¶æ£€æŸ¥ç»´åº¦
        print("\n2. åˆ›å»º FeatureGraph å¹¶æ£€æŸ¥ç»´åº¦æ¨æ–­...")
        data_source = create_mock_data_source(num_events=50)
        table = data_source.load_branches(["met", "Jet_pt", "is_signal"])

        engine = ExpressionEngine()
        feature_graph = FeatureGraph.from_feature_defs(feature_defs, engine, enable_cache=False)
        feature_graph.compile()
        feature_graph.fit(table)

        output_spec = feature_graph.output_spec()
        print(f"   output_spec: {output_spec}")

        # æ¨¡æ‹Ÿ PipelineOrchestrator.setup_model() çš„ç»´åº¦æ¨æ–­é€»è¾‘
        print("\n3. æ¨¡æ‹Ÿç»´åº¦è‡ªåŠ¨æ¨æ–­...")
        model_kwargs = pipeline_config["model"]["params"].copy()

        if "event" in output_spec:
            inferred_event_dim = output_spec["event"]["dim"]
            model_kwargs["event_input_dim"] = inferred_event_dim
            print(f"   âœ“ è‡ªåŠ¨æ¨æ–­ event_input_dim={inferred_event_dim}")

        if "object" in output_spec:
            inferred_object_dim = output_spec["object"]["dim"]
            model_kwargs["object_input_dim"] = inferred_object_dim
            print(f"   âœ“ è‡ªåŠ¨æ¨æ–­ object_input_dim={inferred_object_dim}")

        # åˆ›å»ºæ¨¡å‹
        print("\n4. ä½¿ç”¨æ¨æ–­çš„ç»´åº¦åˆ›å»ºæ¨¡å‹...")
        model = get_model("mlp_classifier", **model_kwargs)
        print(f"   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹å‚æ•°: event_input_dim={model_kwargs.get('event_input_dim')}, object_input_dim={model_kwargs.get('object_input_dim')}")

        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("\n5. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        batch = feature_graph.build_batch(table)
        print_batch_info(batch, "æ¨¡å‹è¾“å…¥ Batch")

        model.eval()
        with torch.no_grad():
            output = model(batch)
        print(f"   è¾“å‡º shape: {output.shape}")
        print("   âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")

        print("\nâœ“ æµ‹è¯• 4 å®Œæˆï¼šPipelineOrchestrator è‡ªåŠ¨ç»´åº¦æ¨æ–­")


def test_regression_with_new_architecture():
    """æµ‹è¯•å›å½’ä»»åŠ¡ä½¿ç”¨æ–°æ¶æ„"""
    print_section("æµ‹è¯• 5: å›å½’ä»»åŠ¡ï¼ˆæ–°æ¶æ„ï¼‰")

    # åˆ›å»º FeatureGraphï¼ˆevent + object ç‰¹å¾ï¼‰
    feature_defs = {
        "met": {
            "expr": "met",
            "type": "event",
            "dtype": "float32",
        },
        "jet_pt": {
            "expr": "Jet_pt",
            "type": "object",
            "dtype": "float32",
            "padding": {
                "max_length": 10,
                "mode": "constant",
            },
        },
    }

    engine = ExpressionEngine()
    feature_graph = FeatureGraph.from_feature_defs(feature_defs, engine, enable_cache=False)
    feature_graph.compile()

    # åˆ›å»ºæ•°æ®
    data_source = create_mock_data_source(num_events=50)
    table = data_source.load_branches(["met", "Jet_pt", "is_signal"])

    # æ‹Ÿåˆ FeatureGraph
    print("\n1. æ‹Ÿåˆ FeatureGraph...")
    feature_graph.fit(table)

    # æ£€æŸ¥ output_spec
    output_spec = feature_graph.output_spec()
    print(f"   output_spec: {output_spec}")

    # åˆ›å»ºå›å½’æ¨¡å‹
    print("\n2. åˆ›å»ºå›å½’æ¨¡å‹...")
    model = get_model(
        "mlp_regressor",
        event_input_dim=output_spec["event"]["dim"],
        object_input_dim=output_spec["object"]["dim"],
        embed_dim=64,
        hidden_dims=[32, 16],
        num_outputs=1,
        object_pooling_mode="mean",
    )
    print(f"   âœ“ å›å½’æ¨¡å‹åˆ›å»ºæˆåŠŸ")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n3. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
    batch = feature_graph.build_batch(table)
    print_batch_info(batch, "æ¨¡å‹è¾“å…¥ Batch")

    model.eval()
    with torch.no_grad():
        output = model(batch)
    print(f"   è¾“å‡º shape: {output.shape}")
    print(f"   è¾“å‡ºå€¼ï¼ˆå‰5ä¸ªï¼‰: {output[:5].squeeze()}")
    assert output.shape == (len(table), 1), f"è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ ({len(table)}, 1)ï¼Œå®é™…æ˜¯ {output.shape}"
    print("   âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")

    # æµ‹è¯•è®­ç»ƒ
    print("\n4. æµ‹è¯•è®­ç»ƒæµç¨‹...")
    data_config = DataConfig(
        selection=None,
        labels={"type": "simple", "value": ["is_signal"]},
    )
    dataset = HEPDataset(
        data_source=data_source,
        data_config=data_config,
        feature_graph=feature_graph,
        for_training=True,
        shuffle=False,
    )

    loader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn, num_workers=0)

    trainer = Trainer(
        model=model,
        train_loader=loader,
        loss_fn=torch.nn.MSELoss(),
        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),
        device=torch.device("cpu"),
        task_type="regression",
    )

    trainer.fit(num_epochs=1)
    print(f"   âœ“ è®­ç»ƒå®Œæˆ")

    print("\nâœ“ æµ‹è¯• 5 å®Œæˆï¼šå›å½’ä»»åŠ¡ï¼ˆæ–°æ¶æ„ï¼‰")


def test_specific_dimensions():
    """æµ‹è¯•ç‰¹å®šç»´åº¦é…ç½®ï¼ševent_input_dim=5, object_input_dim=10"""
    print_section("æµ‹è¯• 6: ç‰¹å®šç»´åº¦é…ç½®æ¼”ç¤º (event_input_dim=5, object_input_dim=10)")

    print("\n" + "=" * 80)
    print("  ğŸ“Š æ•°æ®æµåˆ†æï¼ševent_input_dim=5, object_input_dim=10")
    print("=" * 80)

    # ç›´æ¥åˆ›å»ºæ¨¡å‹æ¼”ç¤ºç»´åº¦é…ç½®
    event_input_dim = 5
    object_input_dim = 10
    embed_dim = 64

    print(f"\n1. æ¨¡å‹é…ç½®ï¼š")
    print(f"   event_input_dim = {event_input_dim}  # Event-level ç‰¹å¾æ•°é‡")
    print(f"   object_input_dim = {object_input_dim}  # Object-level ç‰¹å¾æ•°é‡ï¼ˆæ¯ä¸ªå¯¹è±¡ï¼‰")
    print(f"   embed_dim = {embed_dim}  # Embedding ç»´åº¦")
    print(f"   object_pooling_mode = 'mean'  # Object ç‰¹å¾æ± åŒ–æ–¹å¼")

    print(f"\n2. åˆ›å»ºæ¨¡å‹...")
    model = get_model(
        "mlp_classifier",
        event_input_dim=event_input_dim,
        object_input_dim=object_input_dim,
        embed_dim=embed_dim,
        hidden_dims=[128, 64, 32],
        num_classes=2,
        object_pooling_mode="mean",
    )
    print(f"   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")

    print(f"\n3. æ•°æ®æµè¯¦è§£ï¼š")
    print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"   â”‚ Batch è¾“å…¥ (batch_size = B)                                 â”‚")
    print(f"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"   â”‚ event:  (B, {event_input_dim})  # Event-level ç‰¹å¾          â”‚")
    print(f"   â”‚ object: (B, N, {object_input_dim})  # Object-level ç‰¹å¾     â”‚")
    print(f"   â”‚ mask:   (B, N)  # Maskï¼ˆTrue=æœ‰æ•ˆï¼ŒFalse=paddingï¼‰         â”‚")
    print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"                          â†“")
    print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"   â”‚ Event Embedding                                             â”‚")
    print(f"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"   â”‚ Linear({event_input_dim}, {embed_dim}) + Activation          â”‚")
    print(f"   â”‚ â†’ event_emb: (B, {embed_dim})                                â”‚")
    print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"                          â†“")
    print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"   â”‚ Object Embedding                                            â”‚")
    print(f"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"   â”‚ Linear({object_input_dim}, {embed_dim}) + Activation          â”‚")
    print(f"   â”‚ â†’ object_emb: (B, N, {embed_dim})                           â”‚")
    print(f"   â”‚ â†’ Pooling (mean with mask)                                  â”‚")
    print(f"   â”‚ â†’ object_emb_pooled: (B, {embed_dim})                       â”‚")
    print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"                          â†“")
    print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"   â”‚ Fusion (Concatenate)                                        â”‚")
    print(f"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"   â”‚ Concat([event_emb, object_emb_pooled], dim=-1)             â”‚")
    print(f"   â”‚ â†’ fused: (B, {embed_dim * 2})  # {embed_dim} + {embed_dim} = {embed_dim * 2}  â”‚")
    print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"                          â†“")
    print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"   â”‚ MLP Backbone                                                â”‚")
    print(f"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"   â”‚ Linear({embed_dim * 2}, 128) â†’ ReLU â†’ Dropout               â”‚")
    print(f"   â”‚ Linear(128, 64) â†’ ReLU â†’ Dropout                            â”‚")
    print(f"   â”‚ Linear(64, 32) â†’ ReLU â†’ Dropout                             â”‚")
    print(f"   â”‚ Linear(32, 2)  # num_classes                                â”‚")
    print(f"   â”‚ â†’ output: (B, 2)                                            â”‚")
    print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æ¼”ç¤º
    print(f"\n4. ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºå‰å‘ä¼ æ’­...")
    batch_size = 8
    max_objects = 10

    # åˆ›å»ºæ¨¡æ‹Ÿ batch
    mock_batch = {
        "event": torch.randn(batch_size, event_input_dim),
        "object": torch.randn(batch_size, max_objects, object_input_dim),
        "mask": torch.ones(batch_size, max_objects, dtype=torch.bool),
    }

    print_batch_info(mock_batch, "æ¨¡æ‹Ÿ Batch è¾“å…¥")

    model.eval()
    with torch.no_grad():
        output = model(mock_batch)

    print(f"\n   è¾“å‡º shape: {output.shape}")
    print(f"   è¾“å‡ºå€¼ï¼ˆå‰3ä¸ªæ ·æœ¬ï¼‰:")
    for i in range(min(3, batch_size)):
        print(f"      æ ·æœ¬ {i}: {output[i].cpu().numpy()}")

    print(f"\n5. å‚æ•°ç»Ÿè®¡ï¼š")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")

    # è®¡ç®—å„å±‚çš„å‚æ•°é‡
    print(f"\n   å„å±‚å‚æ•°é‡åˆ†è§£ï¼š")
    print(f"   - Event Embedding (Linear({event_input_dim}, {embed_dim})): {event_input_dim * embed_dim + embed_dim:,}")
    print(f"   - Object Embedding (Linear({object_input_dim}, {embed_dim})): {object_input_dim * embed_dim + embed_dim:,}")
    print(f"   - MLP Backbone (128â†’64â†’32â†’2): ~{(embed_dim * 2 * 128 + 128 * 64 + 64 * 32 + 32 * 2):,}")

    print("\nâœ“ æµ‹è¯• 6 å®Œæˆï¼šç‰¹å®šç»´åº¦é…ç½®æ¼”ç¤º")


def test_real_root_file_regression():
    """ä½¿ç”¨çœŸå® ROOT æ–‡ä»¶æµ‹è¯•å›å½’ä»»åŠ¡ï¼ˆéœ€è¦æœ¬åœ° ROOT æ–‡ä»¶ï¼ŒCI ä¸­è·³è¿‡ï¼‰"""
    print_section("æµ‹è¯• 7: çœŸå® ROOT æ–‡ä»¶å›å½’ä»»åŠ¡")

    # ROOT æ–‡ä»¶è·¯å¾„
    root_file_path = ".../merge_ss_0006.root"
    tree_name = "tree"  # æ ¹æ® ROOT æ–‡ä»¶ä¸­çš„å®é™… tree åç§°

    # åœ¨ CI ç¯å¢ƒä¸­æˆ–æ–‡ä»¶ä¸å­˜åœ¨æ—¶è·³è¿‡æµ‹è¯•
    if os.getenv("CI") == "true" or not os.path.exists(root_file_path):
        pytest.skip(f"è·³è¿‡æµ‹è¯•ï¼šéœ€è¦æœ¬åœ° ROOT æ–‡ä»¶ï¼ˆåœ¨ CI ç¯å¢ƒä¸­æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼‰\næ–‡ä»¶è·¯å¾„: {root_file_path}")

    print(f"\n1. åŠ è½½ ROOT æ–‡ä»¶...")
    print(f"   æ–‡ä»¶è·¯å¾„: {root_file_path}")
    print(f"   Tree åç§°: {tree_name}")

    try:
        # åˆ›å»ºæ•°æ®æºï¼ˆåªåŠ è½½ä¸€å°éƒ¨åˆ†æ•°æ®ç”¨äºæµ‹è¯•ï¼‰
        data_source = DataSourceFactory.create(
            root_file_path,
            treename=tree_name,
            load_range=(0, 0.1),  # åªåŠ è½½å‰ 10% çš„æ•°æ®
        )
        print(f"   âœ“ æ•°æ®æºåˆ›å»ºæˆåŠŸ")
        print(f"   äº‹ä»¶æ•°: {data_source.get_num_events()}")

        # è·å–å¯ç”¨åˆ†æ”¯
        available_branches = data_source.get_available_branches()
        print(f"   å¯ç”¨åˆ†æ”¯æ•°: {len(available_branches)}")

    except Exception as e:
        pytest.skip(f"æ— æ³•åŠ è½½ ROOT æ–‡ä»¶: {e}")

    # åˆ›å»º FeatureGraph
    print("\n2. åˆ›å»º FeatureGraph...")
    feature_defs = {
        # Event-level ç‰¹å¾
        "jet_phi": {
            "expr": "jet_phi",
            "type": "event",
            "dtype": "float32",
        },
        "jet_eta": {
            "expr": "jet_eta",
            "type": "event",
            "dtype": "float32",
        },
        # Object-level ç‰¹å¾
        "part_d0": {
            "expr": "part_d0",
            "type": "object",
            "dtype": "float32",
            "padding": {
                "max_length": 50,  # æ ¹æ®å®é™…æ•°æ®è°ƒæ•´
                "mode": "constant",
            },
        },
        "part_isKLong": {
            "expr": "part_isKLong",
            "type": "object",
            "dtype": "float32",  # bool è½¬ä¸º float32
            "padding": {
                "max_length": 50,
                "mode": "constant",
            },
        },
        "part_deltaR": {
            "expr": "part_deltaR",
            "type": "object",
            "dtype": "float32",
            "padding": {
                "max_length": 50,
                "mode": "constant",
            },
        },
    }

    engine = ExpressionEngine()
    feature_graph = FeatureGraph.from_feature_defs(feature_defs, engine, enable_cache=False)
    feature_graph.compile()
    print("   âœ“ FeatureGraph åˆ›å»ºæˆåŠŸ")

    # åŠ è½½æ•°æ®
    print("\n3. åŠ è½½æ•°æ®å¹¶æ‹Ÿåˆ FeatureGraph...")
    table = data_source.load_branches(
        [
            "jet_phi",
            "jet_eta",
            "jet_energy",
            "part_d0",
            "part_isKLong",
            "part_deltaR",
        ]
    )

    # æ£€æŸ¥åŸå§‹æ•°æ®ç±»å‹ï¼ˆç”¨äºéªŒè¯ï¼‰
    print(f"   æ•°æ®è¡¨å­—æ®µ: {list(table.fields)}")
    print(f"   äº‹ä»¶æ•°: {len(table)}")
    if "part_isKLong" in table.fields:
        sample_val = table["part_isKLong"][0]
        if isinstance(sample_val, ak.Array):
            print(f"   part_isKLong åŸå§‹ç±»å‹: vector<bool> (jagged array)")
        else:
            print(f"   part_isKLong åŸå§‹ç±»å‹: {type(sample_val)}")

    # æ³¨æ„ï¼šä¸æ‰‹åŠ¨è½¬æ¢ç±»å‹ï¼Œè®© FeatureProcessor è‡ªåŠ¨å¤„ç† bool -> float32 è½¬æ¢
    print(f"   âœ“ å°†ä½¿ç”¨ FeatureProcessor è‡ªåŠ¨å¤„ç†ç±»å‹è½¬æ¢ï¼ˆbool -> float32ï¼‰")

    # æ‹Ÿåˆ FeatureGraphï¼ˆä¼šè§¦å‘ FeatureProcessor çš„ç±»å‹è½¬æ¢ï¼‰
    print("\n4. æ‹Ÿåˆ FeatureGraphï¼ˆæµ‹è¯•è‡ªåŠ¨ç±»å‹è½¬æ¢ï¼‰...")
    feature_graph.fit(table)
    print("   âœ“ FeatureGraph æ‹ŸåˆæˆåŠŸ")

    # éªŒè¯ç±»å‹è½¬æ¢ï¼šæ£€æŸ¥å¤„ç†åçš„ part_isKLong ç±»å‹
    if "part_isKLong" in table.fields:
        # æ„å»ºä¸€ä¸ªæµ‹è¯• batch æ¥æŸ¥çœ‹å¤„ç†åçš„ç±»å‹
        test_batch = feature_graph.build_batch(table[:1])  # åªå¤„ç†ç¬¬ä¸€ä¸ªäº‹ä»¶ç”¨äºæ£€æŸ¥
        if "object" in test_batch:
            object_tensor = test_batch["object"]
            print(f"   âœ“ ç±»å‹è½¬æ¢éªŒè¯ï¼š")
            print(f"     - å¤„ç†åçš„ object tensor dtype: {object_tensor.dtype}")
            print(f"     - object tensor shape: {object_tensor.shape}")
            # part_isKLong åº”è¯¥æ˜¯ object ç‰¹å¾çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥ object tensor åº”è¯¥åŒ…å«å®ƒ
            print(f"     - ç¡®è®¤ï¼špart_isKLong (bool) å·²è‡ªåŠ¨è½¬æ¢ä¸º float32 å¹¶åŒ…å«åœ¨ object tensor ä¸­")

    # æ£€æŸ¥ output_spec
    print("\n5. æ£€æŸ¥ FeatureGraph.output_spec()...")
    output_spec = feature_graph.output_spec()
    print(f"   output_spec: {output_spec}")

    if "event" in output_spec:
        print(f"   event dim: {output_spec['event']['dim']}")
        print(f"   event features: {output_spec['event']['features']}")
    if "object" in output_spec:
        print(f"   object dim: {output_spec['object']['dim']}")
        print(f"   object max_length: {output_spec['object']['max_length']}")
        print(f"   object features: {output_spec['object']['features']}")

    # åˆ›å»ºæ•°æ®é…ç½®
    print("\n6. åˆ›å»º DataConfig...")
    data_config = DataConfig(
        treename=tree_name,
        selection=None,
        labels={"type": "complex", "value": {"_label_": "jet_energy"}},  # å›å½’ä»»åŠ¡ä½¿ç”¨ complex ç±»å‹ï¼Œvalue å¿…é¡»æ˜¯å­—å…¸
    )
    print("   âœ“ DataConfig åˆ›å»ºæˆåŠŸ")

    # åˆ›å»ºæ•°æ®é›†
    print("\n7. åˆ›å»º HEPDataset...")
    dataset = HEPDataset(
        data_source=data_source,
        data_config=data_config,
        feature_graph=feature_graph,
        for_training=True,
        shuffle=False,
    )
    print("   âœ“ HEPDataset åˆ›å»ºæˆåŠŸ")

    # åˆ›å»º DataLoader
    print("\n8. åˆ›å»º DataLoader...")
    loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, num_workers=0)

    # æ£€æŸ¥ batch æ ¼å¼
    sample_batch = next(iter(loader))
    print_batch_info(sample_batch, "DataLoader è¾“å‡ºçš„ Batch")

    # åˆ›å»ºå›å½’æ¨¡å‹
    print("\n9. åˆ›å»ºå›å½’æ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ç»´åº¦ï¼‰...")
    event_input_dim = output_spec.get("event", {}).get("dim")
    object_input_dim = output_spec.get("object", {}).get("dim")

    model = get_model(
        "mlp_regressor",
        event_input_dim=event_input_dim,
        object_input_dim=object_input_dim,
        embed_dim=64,
        hidden_dims=[128, 64, 32],
        num_outputs=1,
        object_pooling_mode="mean",
    )
    print(f"   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   event_input_dim: {event_input_dim}")
    print(f"   object_input_dim: {object_input_dim}")
    print(f"   embed_dim: 64")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n10. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
    model.eval()
    with torch.no_grad():
        output = model(sample_batch)
    print(f"   è¾“å…¥ event shape: {sample_batch['event'].shape if 'event' in sample_batch else 'N/A'}")
    print(f"   è¾“å…¥ object shape: {sample_batch['object'].shape if 'object' in sample_batch else 'N/A'}")
    print(f"   è¾“å…¥ mask shape: {sample_batch['mask'].shape if 'mask' in sample_batch else 'N/A'}")
    print(f"   æ ‡ç­¾ shape: {sample_batch['_label_'].shape if '_label_' in sample_batch else 'N/A'}")
    print(f"   è¾“å‡º shape: {output.shape}")
    print(f"   è¾“å‡ºå€¼èŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
    print(f"   è¾“å‡ºå‡å€¼: {output.mean().item():.4f}")
    if "_label_" in sample_batch:
        labels = sample_batch["_label_"]
        # å¯¹äº int64 ç±»å‹çš„æ ‡ç­¾ï¼Œéœ€è¦è½¬æ¢ä¸º float æ‰èƒ½è®¡ç®— mean
        labels_float = labels.float()
        print(f"   æ ‡ç­¾å€¼èŒƒå›´: [{labels_float.min().item():.4f}, {labels_float.max().item():.4f}]")
        print(f"   æ ‡ç­¾å‡å€¼: {labels_float.mean().item():.4f}")

    # æµ‹è¯•è®­ç»ƒ
    print("\n11. æµ‹è¯•è®­ç»ƒæµç¨‹ï¼ˆ1ä¸ª epochï¼‰...")
    trainer = Trainer(
        model=model,
        train_loader=loader,
        loss_fn=torch.nn.MSELoss(),
        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),
        device=torch.device("cpu"),
        task_type="regression",
    )

    history = trainer.fit(num_epochs=1)
    print(f"   âœ“ è®­ç»ƒå®Œæˆ")
    print(f"   History keys: {list(history.keys())}")
    if "history" in history:
        train_loss = history["history"].get("train_loss", [])
        val_loss = history["history"].get("val_loss", [])
        if train_loss:
            print(f"   è®­ç»ƒ loss: {train_loss[-1]:.6f}")
        if val_loss:
            print(f"   éªŒè¯ loss: {val_loss[-1]:.6f}")

    # æµ‹è¯•è¯„ä¼°
    print("\n12. æµ‹è¯•è¯„ä¼°æµç¨‹...")
    evaluator = Evaluator(task_type="regression")
    metrics = evaluator.evaluate(model, loader, loss_fn=torch.nn.MSELoss(), device=torch.device("cpu"))
    print(f"   è¯„ä¼°æŒ‡æ ‡: {metrics}")
    print("   âœ“ è¯„ä¼°æµ‹è¯•é€šè¿‡")

    print("\nâœ“ æµ‹è¯• 7 å®Œæˆï¼šçœŸå® ROOT æ–‡ä»¶å›å½’ä»»åŠ¡")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("  æ–°æ¶æ„é›†æˆæµ‹è¯•")
    print("=" * 80)
    print("\næµ‹è¯•å†…å®¹ï¼š")
    print("  1. åªæœ‰ Event-Level ç‰¹å¾")
    print("  2. åªæœ‰ Object-Level ç‰¹å¾")
    print("  3. Event + Object ç‰¹å¾")
    print("  4. PipelineOrchestrator è‡ªåŠ¨ç»´åº¦æ¨æ–­")
    print("  5. å›å½’ä»»åŠ¡ï¼ˆæ–°æ¶æ„ï¼‰")
    print("  6. ç‰¹å®šç»´åº¦é…ç½® (event_input_dim=5, object_input_dim=10)")
    print("  7. çœŸå® ROOT æ–‡ä»¶å›å½’ä»»åŠ¡")
    print("=" * 80)

    try:
        test_only_event_features()
        test_only_object_features()
        test_both_event_and_object_features()
        test_pipeline_orchestrator_auto_inference()
        test_regression_with_new_architecture()
        test_specific_dimensions()
        test_real_root_file_regression()

        print("\n" + "=" * 80)
        print("  âœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
